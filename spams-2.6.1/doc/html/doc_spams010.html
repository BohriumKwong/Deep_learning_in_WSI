<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="hevea 2.23">

<META name="Author" content="Julien Mairal">
<link rel="stylesheet" href="doc_spams.css"><link rel="stylesheet" type="text/css" href="doc_spams.css">
<title>References</title>
</head>
<body>
<a href="doc_spams009.html"><img src="previous_motif.gif" alt="Previous"></a>
<a href="index.html"><img src="contents_motif.gif" alt="Up"></a>
<hr>
<h2 class="section" id="sec71">References</h2>
<dl class="thebibliography"><dt class="dt-thebibliography">
<a id="beck">[1]</a></dt><dd class="dd-thebibliography">
A. Beck and M. Teboulle.
A fast iterative shrinkage-thresholding algorithm for linear inverse
problems.
<em>SIAM Journal on Imaging Sciences</em>, 2(1):183–202, 2009.</dd><dt class="dt-thebibliography"><a id="borwein">[2]</a></dt><dd class="dd-thebibliography">
J. M. Borwein and A. S. Lewis.
<em>Convex analysis and nonlinear optimization: Theory and
examples</em>.
Springer, 2006.</dd><dt class="dt-thebibliography"><a id="brucker">[3]</a></dt><dd class="dd-thebibliography">
P. Brucker.
An O(n) algorithm for quadratic knapsack problems.
3:163–166, 1984.</dd><dt class="dt-thebibliography"><a id="candes4">[4]</a></dt><dd class="dd-thebibliography">
E. J. Candès, M. Wakin, and S. Boyd.
Enhancing sparsity by reweighted l1 minimization.
<em>Journal of Fourier Analysis and Applications</em>, 14:877–905,
2008.</dd><dt class="dt-thebibliography"><a id="cherkassky">[5]</a></dt><dd class="dd-thebibliography">
B. V. Cherkassky and A. V. Goldberg.
On implementing the push-relabel method for the maximum flow problem.
<em>Algorithmica</em>, 19(4):390–410, 1997.</dd><dt class="dt-thebibliography"><a id="cotter">[6]</a></dt><dd class="dd-thebibliography">
S. F. Cotter, J. Adler, B. Rao, and K. Kreutz-Delgado.
Forward sequential algorithms for best basis selection.
In <em>IEEE Proceedings of Vision Image and Signal Processing</em>,
pages 235–244, 1999.</dd><dt class="dt-thebibliography"><a id="Cut94">[7]</a></dt><dd class="dd-thebibliography">
A. Cutler and L. Breiman.
Archetypal analysis.
<em>Technometrics</em>, 36(4):338–347, 1994.</dd><dt class="dt-thebibliography"><a id="duchi">[8]</a></dt><dd class="dd-thebibliography">
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
Efficient projections onto the ℓ<sub>1</sub>-ball for learning in high
dimensions.
In <em>Proceedings of the International Conference on Machine
Learning (ICML)</em>, 2008.</dd><dt class="dt-thebibliography"><a id="efron">[9]</a></dt><dd class="dd-thebibliography">
B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression.
<em>Annals of statistics</em>, 32(2):407–499, 2004.</dd><dt class="dt-thebibliography"><a id="friedman">[10]</a></dt><dd class="dd-thebibliography">
J. Friedman, T. Hastie, H. Hölfling, and R. Tibshirani.
Pathwise coordinate optimization.
<em>Annals of statistics</em>, 1(2):302–332, 2007.</dd><dt class="dt-thebibliography"><a id="Friedman2010">[11]</a></dt><dd class="dd-thebibliography">
J. Friedman, T. Hastie, and R. Tibshirani.
A note on the group lasso and a sparse group lasso.
Technical report, Preprint arXiv:1001.0736, 2010.</dd><dt class="dt-thebibliography"><a id="fu">[12]</a></dt><dd class="dd-thebibliography">
W. J. Fu.
Penalized regressions: The bridge versus the Lasso.
<em>Journal of computational and graphical statistics</em>, 7:397–416,
1998.</dd><dt class="dt-thebibliography"><a id="goldberg">[13]</a></dt><dd class="dd-thebibliography">
A. V. Goldberg and R. E. Tarjan.
A new approach to the maximum flow problem.
In <em>Proc. of ACM Symposium on Theory of Computing</em>, pages
136–146, 1986.</dd><dt class="dt-thebibliography"><a id="hoyer">[14]</a></dt><dd class="dd-thebibliography">
P. O. Hoyer.
Non-negative sparse coding.
In <em>Proc. IEEE Workshop on Neural Networks for Signal
Processing</em>, 2002.</dd><dt class="dt-thebibliography"><a id="jenatton3">[15]</a></dt><dd class="dd-thebibliography">
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal methods for sparse hierarchical dictionary learning.
In <em>Proceedings of the International Conference on Machine
Learning (ICML)</em>, 2010.</dd><dt class="dt-thebibliography"><a id="jenatton4">[16]</a></dt><dd class="dd-thebibliography">
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach.
Proximal methods for hierarchical sparse coding.
<em>Journal of Machine Learning Research</em>, 12:2297–2334, 2011.</dd><dt class="dt-thebibliography"><a id="lee2">[17]</a></dt><dd class="dd-thebibliography">
D. D. Lee and H. S. Seung.
Algorithms for non-negative matrix factorization.
In <em>Advances in Neural Information Processing Systems</em>, 2001.</dd><dt class="dt-thebibliography"><a id="maculan">[18]</a></dt><dd class="dd-thebibliography">
N. Maculan and J. R. G. Galdino de Paula.
A linear-time median-finding algorithm for projecting a vector on the
simplex of Rn.
<em>Operations research letters</em>, 8(4):219–222, 1989.</dd><dt class="dt-thebibliography"><a id="mairal11">[19]</a></dt><dd class="dd-thebibliography">
J. Mairal.
<em>Sparse coding for machine learning, image processing and
computer vision</em>.
PhD thesis, Ecole Normale Supérieure, Cachan, 2010.</dd><dt class="dt-thebibliography"><a id="mairal7">[20]</a></dt><dd class="dd-thebibliography">
J. Mairal, F. Bach, J. Ponce, and G. Sapiro.
Online dictionary learning for sparse coding.
In <em>Proceedings of the International Conference on Machine
Learning (ICML)</em>, 2009.</dd><dt class="dt-thebibliography"><a id="mairal9">[21]</a></dt><dd class="dd-thebibliography">
J. Mairal, F. Bach, J. Ponce, and G. Sapiro.
Online learning for matrix factorization and sparse coding.
<em>Journal of Machine Learning Research</em>, 11:19–60, 2010.</dd><dt class="dt-thebibliography"><a id="mairal10">[22]</a></dt><dd class="dd-thebibliography">
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach.
Network flow algorithms for structured sparsity.
In <em>Advances in Neural Information Processing Systems</em>, 2010.</dd><dt class="dt-thebibliography"><a id="mairal13">[23]</a></dt><dd class="dd-thebibliography">
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach.
Convex and network flow optimization for structured sparsity.
<em>Journal of Machine Learning Research</em>, 12:2649–2689, 2011.</dd><dt class="dt-thebibliography"><a id="mairal14">[24]</a></dt><dd class="dd-thebibliography">
J. Mairal and B. Yu.
Supervised feature selection in graphs with path coding penalties and
network flows.
<em>Journal of Machine Learning Research</em>, 2013.</dd><dt class="dt-thebibliography"><a id="mairal16">[25]</a></dt><dd class="dd-thebibliography">
Julien Mairal.
Optimization with first-order surrogate functions.
In <em>International Conference on Machine Learning (ICML)</em>, 2013.</dd><dt class="dt-thebibliography"><a id="mairal15">[26]</a></dt><dd class="dd-thebibliography">
Julien Mairal.
Stochastic majorization-minimization algorithms for large-scale
optimization.
In <em>Advances in Neural Information Processing Systems (NIPS)</em>,
2013.</dd><dt class="dt-thebibliography"><a id="mallat4">[27]</a></dt><dd class="dd-thebibliography">
S. Mallat and Z. Zhang.
Matching pursuit in a time-frequency dictionary.
<em>IEEE Transactions on Signal Processing</em>, 41(12):3397–3415,
1993.</dd><dt class="dt-thebibliography"><a id="meinshausen">[28]</a></dt><dd class="dd-thebibliography">
N. Meinshausen and P. Buehlmann.
Stability selection.
Technical report.
ArXiv:0809.2932.</dd><dt class="dt-thebibliography"><a id="obozinski">[29]</a></dt><dd class="dd-thebibliography">
G. Obozinski, B. Taskar, and M.I. Jordan.
Joint covariate selection and joint subspace selection for multiple
classification problems.
<em>Statistics and Computing</em>, pages 1–22.</dd><dt class="dt-thebibliography"><a id="osborne">[30]</a></dt><dd class="dd-thebibliography">
M. R. Osborne, B. Presnell, and B. A. Turlach.
On the Lasso and its dual.
<em>Journal of Computational and Graphical Statistics</em>,
9(2):319–37, 2000.</dd><dt class="dt-thebibliography"><a id="sprechmann">[31]</a></dt><dd class="dd-thebibliography">
P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar.
Collaborative hierarchical sparse modeling.
Technical report, 2010.
Preprint arXiv:1003.0400v1.</dd><dt class="dt-thebibliography"><a id="tibshirani2">[32]</a></dt><dd class="dd-thebibliography">
R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight.
Sparsity and smoothness via the fused lasso.
<em>Journal of the Royal Statistical Society Series B</em>,
67(1):91–108, 2005.</dd><dt class="dt-thebibliography"><a id="tropp3">[33]</a></dt><dd class="dd-thebibliography">
J. A. Tropp.
Algorithms for simultaneous sparse approximation. part ii: Convex
relaxation.
<em>Signal Processing, special issue "Sparse approximations in
signal and image processing"</em>, 86:589–602, April 2006.</dd><dt class="dt-thebibliography"><a id="tropp2">[34]</a></dt><dd class="dd-thebibliography">
J. A. Tropp, A. C. Gilbert, and M. J. Strauss.
Algorithms for simultaneous sparse approximation. part i: Greedy
pursuit.
<em>Signal Processing, special issue "sparse approximations in
signal and image processing"</em>, 86:572–588, April 2006.</dd><dt class="dt-thebibliography"><a id="weisberg">[35]</a></dt><dd class="dd-thebibliography">
S. Weisberg.
<em>Applied Linear Regression</em>.
Wiley, New York, 1980.</dd><dt class="dt-thebibliography"><a id="wu">[36]</a></dt><dd class="dd-thebibliography">
T. T. Wu and K. Lange.
Coordinate descent algorithms for Lasso penalized regression.
<em>Annals of Applied Statistics</em>, 2(1):224–244, 2008.</dd><dt class="dt-thebibliography"><a id="ChenCVPR">[37]</a></dt><dd class="dd-thebibliography">
J. Mairal Y. Chen and Z. Harchaoui.
Fast and robust archetypal analysis for representation learning.
In <em>Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, 2014.</dd><dt class="dt-thebibliography"><a id="yuan">[38]</a></dt><dd class="dd-thebibliography">
M. Yuan and Y. Lin.
Model selection and estimation in regression with grouped variables.
<em>Journal of the Royal Statistical Society Series B</em>, 68:49–67,
2006.</dd><dt class="dt-thebibliography"><a id="zou">[39]</a></dt><dd class="dd-thebibliography">
H. Zou and T. Hastie.
Regularization and variable selection via the elastic net.
<em>Journal of the Royal Statistical Society Series B</em>,
67(2):301–320, 2005.</dd></dl><hr>
<a href="doc_spams009.html"><img src="previous_motif.gif" alt="Previous"></a>
<a href="index.html"><img src="contents_motif.gif" alt="Up"></a>
</body>
</html>
